{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 571 - Supervised Learning I\n",
    "# Lab 3: Text classification and hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Submission guidelines](#sg)\n",
    "- [Introduction](#in)\n",
    "- [Exercise 1: Introducing the dataset and EDA](#1)\n",
    "- [Exercise 2: Preprocessing](#2)\n",
    "- [Exercise 3: Model building](#3)\n",
    "- [Exercise 4: Hyperparameter optimization](#4)\n",
    "- [Exercise 5: Test results](#5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# train test split and cross validation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions <a name=\"si\"></a>\n",
    "<hr>\n",
    "rubric={mechanics:2}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment. \n",
    "\n",
    "To correctly submit this assignment follow the instructions below:\n",
    "\n",
    "- Push your assignment to your GitHub repository. \n",
    "- Add a link to your GitHub repository here: LINK TO YOUR GITHUB REPO \n",
    "- Upload an HTML render of your assignment to Canvas. The last cell of this notebook will help you do that.\n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "\n",
    "[Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "\n",
    "**NOTE: The data you download for use in this lab SHOULD NOT BE PUSHED TO YOUR REPOSITORY. You might be penalised for pushing datasets to your repository. I have seeded the repository with `.gitignore` and hoping that it won't let you push CSVs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3247a4b883a670c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introduction <a name=\"in\"></a>\n",
    "<hr>\n",
    "\n",
    "In this lab, we'll focus on two things:\n",
    "1. Working with text data\n",
    "2. Hyperparameter optimization\n",
    "\n",
    "As this is a quiz week, this lab is a bit lighter compared to other labs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Introducing the dataset and EDA <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "Let's develop our own SMS spam filtering system using Kaggle's [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset). We will use `CountVectorizer` to encode text messages and `SVC` for classification. Download the data CSV in the lab folder. **Sorry for the offensive language in some text messages; it's the reality of such platforms ðŸ˜”. If you are sensitive to such language try not to read the raw messages.** \n",
    "\n",
    "The starter code below reads the CSV assuming that it's present in the current directory and renames columns. As usual do not push the CSV in your repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "sms_df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")\n",
    "sms_df = sms_df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "sms_df = sms_df.rename(columns={\"v1\": \"target\", \"v2\": \"sms\"})\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data splitting \n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split `sms_df` into train (80%) and test splits (20%). \n",
    "2. Examine the first few rows of the train portion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Simple EDA \n",
    "rubric={accuracy:3,reasoning:2}\n",
    "\n",
    "Note that in case of text data the usual EDA is not applicable. In this exercise will carry out some simple EDA to get a sense of the data.  \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. What's the label distribution in the target column? \n",
    "2. What's the average length in characters of text messages? Show the shortest and longest text messages. \n",
    "3. Would you classify `sms` column as a categorical column? Does it make sense to carry out one-hot encoding on this column? Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 1.3 Word clouds\n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Create two word clouds: one for text messages with `spam` target and other for text messages with `non-spam` targets. You may use [the `wordcloud` package](https://github.com/amueller/word_cloud) for this, which you will have to install in your environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Preprocessing <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "We will be encoding the text data using [sklearn's `CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). In this exercise we will explore different options of `CountVectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `CountVectorizer` with default parameters\n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "1. Transform the training data using `CountVectorizer` with default parameters. \n",
    "2. How many features have been created to represent each text message? What does each feature represent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 `CountVectorizer` transformer with `binary=True`\n",
    "rubric={accuracy:2,reasoning:1}\n",
    "\n",
    "1. Transform the training data using `CountVectorizer` with `binary=True`. \n",
    "2. What does each feature represent with this option? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 `max_features` hyperparameter of `CountVectorizer` \n",
    "rubric={accuracy:2,reasoning:5}\n",
    "\n",
    "1. Now pass `max_features=10` to `CountVectorizer` and transform the training data again. \n",
    "2. How many features have been created to represent each text message now?\n",
    "3. Are we likely to overfit or underfit with less number of features? \n",
    "4. What would happen if you encounter a word in test data that's not present in `max_features` of the training data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Model building <a name=\"3\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 `DummyClassifier`\n",
    "rubric={accuracy:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Build a pipeline for feature extraction using `CountVectorizer` with `binary=True` and `DummyClassifier`.\n",
    "2. Report mean cross-validation scores of the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `SVC` with default parameters\n",
    "rubric={accuracy:2,reasoning:1}\n",
    "\n",
    "1. Now build a pipeline for feature extraction using `CountVectorizer` with `binary=True` and `SVC` with default hyperparameters.\n",
    "2. Are you getting better results with `SVC` compared to `DummyClassifier`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Hyperparameter optimization <a name=\"4\"></a>\n",
    "<hr>\n",
    "\n",
    "So far we have been writing loops to try a bunch of different hyperparameter values and pick the one with lowest validation (or cross-validation) error. This operation is so common, in fact, that `scikit-learn` has some [built-in methods](https://scikit-learn.org/stable/modules/grid_search.html) to do it for you. In this exercise, we will focus on two such methods:\n",
    "\n",
    "1. [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
    "2. [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Optimizing `gamma` for RBF SVM \n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "1. Carry out hyperparameter search over `gamma` by sweeping the hyperparameter through the values $10^{-3}, \\ldots, 10^{-1}, 1, \\ldots, 10^{3}$ using [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and 10-fold cross-validation. (The `param_grid` is given in the starter code below.) \n",
    "2. Note the best hyperparameter value and the corresponding best cross-validation score. Compare the score with exercise 3.2 (i.e., `SVC` with default parameters). \n",
    "\n",
    "A few tips about `GridSearchCV`: \n",
    "- The starter code below defines the parameter grid for `gamma` which you can pass to your `GridSearchCV`. Note the syntax `clf__gamma`. We have two steps in our pipeline and we can access the parameters of these steps using __ to go deeper. So `clf__gamma` means `gamma` of `clf` step of the pipeline. \n",
    "- Setting `n_jobs=-1` should speed it up (if you have a multi-core processor).\n",
    "- Similar to `cross_validate` you can pass `return_train_score=True` to your `GridSearchCV` object.\n",
    "- After running `fit` on the `GridSearchCV` object, \n",
    "    - you can access best hyperparameter values with `grid.best_params_` and best scores with `grid.best_score_` if `grid' is your `GridSearchCV` object.\n",
    "    - you can access mean train and cross-validation scores for all hyperparameter values via `grid.cv_results_` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN STARTER CODE\n",
    "\n",
    "param_grid = {\"clf__gamma\": 10.0 ** np.arange(-3, 3)}\n",
    "\n",
    "### END STARTER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Jointly optimizing `C` and `gamma`\n",
    "rubric={accuracy:4,reasoning:2}\n",
    "\n",
    "Let's optimize `C` hyperparameter along with `gamma`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Expand your search to cover the `C` hyperparameter in addition to `gamma`, sweeping the hyperparameter through values $10^{-3}, 10^{-2}, \\ldots, 10^{3}$. Use the same `gamma` values from Exercise 4.1. \n",
    "2. Did you get the same best `gamma` value that you got when optimizing `gamma` only? Why or why not?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optimizing `C`, `gamma`, and `max_features` jointly with `RandomizedSearchCV`\n",
    "rubric={accuracy:4,reasoning:3}\n",
    "\n",
    "In addition to `GridSearchCV` there are other approaches like `RandomizedSearchCV`, which, as its name implies, tries random hyperparameter configurations instead of performing an exhaustive grid search. In this exercise we will explore `RandomizedSearchCV`. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Jointly optimize `C` and `gamma` hyperparameter of SVC RBF, and `max_features` hyperparameter of `CountVectorizer` using [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). For `C` and `gamma` you may use the range of hyperparameter values from the previous questions. For `max_features` hyperparameter use the range of values of your choice. \n",
    "2. Name a situation in which random search would be strongly preferable over a grid search, and briefly explain why.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) 4.4: More sophisticated hyperparameter tuning. \n",
    "rubric={reasoning:1} \n",
    "\n",
    "There are all sorts of software packages that make hyperparameter tuning with `scikit-learn` even more automated. For example:\n",
    "\n",
    "- [hyperopt-sklearn](https://github.com/hyperopt/hyperopt-sklearn)\n",
    "- [auto-sklearn](https://github.com/automl/auto-sklearn)\n",
    "- [SigOptSearchCV](https://sigopt.com/docs/overview/scikit_learn)\n",
    "\n",
    "Give one of these a try and report your thoughts. Or, if you're even more adventurous, you could try a package that isn't tied to `scikit-learn`. There are many options for you to play around with in your ample free time:\n",
    "\n",
    "- [TPOT](https://github.com/rhiever/tpot)\n",
    "- [hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "- [hyperband](https://github.com/zygmuntz/hyperband)\n",
    "- [SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/)\n",
    "- [MOE](https://github.com/Yelp/MOE)\n",
    "- [pybo](https://github.com/mwhoffman/pybo)\n",
    "- [spearmint](https://github.com/HIPS/Spearmint)\n",
    "- [BayesOpt](https://github.com/rmcantin/bayesopt)\n",
    "\n",
    "Note: this list isn't meant to be exhaustive. \n",
    "\n",
    "In other news, the recently announced [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is also supposed to do hyperparameter optimization for you (among many other things it does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Test results <a name=\"5\"></a>\n",
    "<hr>\n",
    "\n",
    "Now that we have done extensive hyperparameter search, it's time to try our best model on the test split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Report test scores\n",
    "rubric={accuracy:2,reasoning:2}\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "1. Fit the `best_estimator_` from the `RandomizedSearchCV` in 4.3 on `X_train` and `y_train` \n",
    "2. Score the fit model on `X_test` and `y_test`. \n",
    "3. Compare your test scores with your best estimator scores from the previous exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [conda env:571]",
   "language": "python",
   "name": "conda-env-571-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
