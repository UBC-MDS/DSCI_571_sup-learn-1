

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Lecture 7: Naive Bayes &#8212; DSCI 571 Supervised Learning I</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/07_naive-Bayes';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 8: Linear Models" href="08_linear-models.html" />
    <link rel="prev" title="Lecture 6: Hyperparameter Optimization and Optimization Bias" href="06_hyperparameter-optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/mds-hex-sticker.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/mds-hex-sticker.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course Information</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../learning_objectives.html">Course Learning Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="00_motivation-course-information.html">Course Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01_terminology-baselines-decision-trees.html">Lecture 1: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="02_ml-fundamentals.html">Lecture 2: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="03_kNNs-SVM-RBF.html">Lecture 3: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="04_preprocessing-pipelines-column-transformer.html">Lecture 4: Preprocessing, <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> Pipeline, <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> ColumnTrasnsformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="05_more-preprocessing-text-feats.html">Lecture 5: More preprocessing, text features</a></li>
<li class="toctree-l1"><a class="reference internal" href="06_hyperparameter-optimization.html">Lecture 6: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 7: Naive Bayes</a></li>
<li class="toctree-l1"><a class="reference internal" href="08_linear-models.html">Lecture 8: Linear Models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Class demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="class_demos/02_class-demo.html">Lecture 2: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_demos/03_class-demo.html">Lecture 3: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_demos/04_class-demo.html">Lecture 4: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_demos/05_class-demo.html">Lectures 5: Class demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="class_demos/08_class-demo.html">Lectures 8: Class demo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../attribution.html">Attributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-MDS/DSCI_571_sup-learn-1" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lectures/07_naive-Bayes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 7: Naive Bayes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-learning-objectives">Lecture learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-spam-ham">Example: spam/ham</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#countvectorizer-to-get-bag-of-words-bow-representation"><code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to get bag-of-words (BOW) representation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-1-decision-trees-quantifying-the-level-of-certainty-of-predictions">Exercise 7.1.1 Decision trees: Quantifying the level of certainty of predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-1-knns-quantifying-the-level-of-certainty-of-predictions">Exercise 7.1.1 KNNs: Quantifying the level of certainty of predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-3-calculating-p-spam-mid-message-and-p-ham-mid-message">Exercise 7.1.3 Calculating <span class="math notranslate nohighlight">\(P(spam \mid message)\)</span> and <span class="math notranslate nohighlight">\(P(ham \mid message)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-probabilities-p-text-spam-and-p-text-ham">The prior probabilities <span class="math notranslate nohighlight">\(P(\text{spam})\)</span> and <span class="math notranslate nohighlight">\(P(\text{ham})\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-text-message"><span class="math notranslate nohighlight">\(P(\text{message})\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-text-message-mid-text-spam"><span class="math notranslate nohighlight">\(P(\text{message} \mid \text{spam})\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svc-classifier-for-spam-detection">SVC classifier for spam detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-naive-bayes-classifier"><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> naive Bayes classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-predict">Naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-prior-probabilities">Computing prior probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-conditional-probabilities">Computing conditional probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-for-the-target-spam">Conditional probabilities for the target spam</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-for-the-target-ham">Conditional probabilities for the target ham</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-naive-bayes-predict">Summary of naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-naive-bayes-predict">Summary naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-2">(iClicker) Exercise 7.2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier-fit">Naive Bayes classifier <code class="docutils literal notranslate"><span class="pre">fit</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probabilities">Prior probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities">Conditional probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laplace-smoothing">Laplace smoothing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simplest-solution-laplace-smoothing">A simplest solution: Laplace smoothing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alpha-hyperparameter-and-the-fundamental-tradeoff"><code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter and the fundamental tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-details-on-the-formulas">(Optional) Details on the formulas</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-with-continuous-features">Naive Bayes with continuous features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-the-generative-model-behind-naive-bayes">(Optional) The generative model behind naive Bayes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-naive-bayes-on-multi-class-problems">(Optional) Naive Bayes on multi-class problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-try-dummyclassifier">Let’s try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-try-multinomialnb">Let’s try <code class="docutils literal notranslate"><span class="pre">MultinomialNB</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-final-remarks">Summary and final remarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-comments-on-naive-bayes">General comments on naive Bayes</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><img alt="" src="../_images/571_banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-7-naive-bayes">
<h1>Lecture 7: Naive Bayes<a class="headerlink" href="#lecture-7-naive-bayes" title="Permalink to this heading">#</a></h1>
<p>UBC Master of Data Science program, 2023-24</p>
<p>Instructor: Varada Kolhatkar</p>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">IPython</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">plotting_functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">GridSearchCV</span><span class="p">,</span>
    <span class="n">cross_val_score</span><span class="p">,</span>
    <span class="n">cross_validate</span><span class="p">,</span>
    <span class="n">train_test_split</span><span class="p">,</span>
<span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span><span class="p">,</span> <span class="n">GaussianNB</span><span class="p">,</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s2">&quot;display.max_colwidth&quot;</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="lecture-learning-objectives">
<h2>Lecture learning objectives<a class="headerlink" href="#lecture-learning-objectives" title="Permalink to this heading">#</a></h2>
<p>From this lecture, you will be able to</p>
<ul class="simple">
<li><p>Explain the naive assumption of naive Bayes.</p></li>
<li><p>Predict targets by hand on toy examples using naive Bayes.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>’s <code class="docutils literal notranslate"><span class="pre">MultiNomialNB</span></code>, <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code>, and <code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code>.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> for different classifiers and explain its usefulness.</p></li>
<li><p>Explain the need of smoothing in naive Bayes.</p></li>
<li><p>Explain how <code class="docutils literal notranslate"><span class="pre">alpha</span></code> controls the fundamental tradeoff.</p></li>
<li><p>Use naive Bayes for multi-class classification.</p></li>
<li><p>Name advantages and disadvantages of naive Bayes.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="motivating-example">
<h2>Motivating example<a class="headerlink" href="#motivating-example" title="Permalink to this heading">#</a></h2>
<section id="example-spam-ham">
<h3>Example: spam/ham<a class="headerlink" href="#example-spam-ham" title="Permalink to this heading">#</a></h3>
<p>Last week in your lab you worked on spam classification using <code class="docutils literal notranslate"><span class="pre">SVC</span></code> classifier.</p>
<p><span class="math notranslate nohighlight">\(X = \begin{bmatrix}\text{&quot;URGENT!! You have been selected to receive a £900 prize reward!&quot;,}\\ \text{&quot;Lol your always so convincing.&quot;}\\ \text{&quot;Congrats! 1 year special cinema pass for 2 is yours. call 09061209465 now!&quot;}\\ \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(y = \begin{bmatrix}\text{spam} \\ \text{ham} \\ \text{spam} \end{bmatrix}\)</span></p>
<p>ML algorithms we have seen so far prefer fixed length numeric input that looks like this:</p>
<p><span class="math notranslate nohighlight">\(X = \begin{bmatrix}1.0 &amp; 4.0 &amp; \ldots &amp; &amp; 3.0\\ 0.0 &amp; 2.0 &amp; \ldots &amp; &amp; 6.0\\ 1.0 &amp; 0.0 &amp; \ldots &amp; &amp; 0.0\\ \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(y = \begin{bmatrix}\text{spam} \\ \text{ham} \\ \text{spam} \end{bmatrix}\)</span></p>
<section id="countvectorizer-to-get-bag-of-words-bow-representation">
<h4><code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to get bag-of-words (BOW) representation<a class="headerlink" href="#countvectorizer-to-get-bag-of-words-bow-representation" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>So we used <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to convert text data into feature vectors where</p>
<ul>
<li><p>each feature is a unique word in the text</p></li>
<li><p>each feature value represents the frequency or presence/absence of the word in the given message</p></li>
</ul>
</li>
</ul>
<!-- <center>    
<img src='./img/bag-of-words.png' width="800">
</center>        
 -->
<p><img alt="" src="../_images/bag-of-words.png" />
<a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/4.pdf">Source</a></p>
<p><br><br><br><br></p>
</section>
</section>
</section>
<section id="questions-for-you">
<h2>❓❓ Questions for you<a class="headerlink" href="#questions-for-you" title="Permalink to this heading">#</a></h2>
<p>We’ve focused on “hard” predictions thus far. Can Decision Trees and KNNs provide scores quantifying the certainty of the model or probabilities? If so, how? Discuss the following two scenarios with your neighbour.</p>
<section id="exercise-7-1-1-decision-trees-quantifying-the-level-of-certainty-of-predictions">
<h3>Exercise 7.1.1 Decision trees: Quantifying the level of certainty of predictions<a class="headerlink" href="#exercise-7-1-1-decision-trees-quantifying-the-level-of-certainty-of-predictions" title="Permalink to this heading">#</a></h3>
<p>Imagine you have trained a decision tree and are provided with several new examples for prediction. How would you determine the model’s level of certainty at each leaf node?</p>
<p><img alt="" src="../_images/spam_toy_decision_tree.png" /></p>
<p><br><br></p>
</section>
<section id="exercise-7-1-1-knns-quantifying-the-level-of-certainty-of-predictions">
<h3>Exercise 7.1.1 KNNs: Quantifying the level of certainty of predictions<a class="headerlink" href="#exercise-7-1-1-knns-quantifying-the-level-of-certainty-of-predictions" title="Permalink to this heading">#</a></h3>
<p>Below are five closest neighbors and their targets extracted from <a class="reference external" href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset">the SMS Spam Collection Dataset</a> for a given <code class="docutils literal notranslate"><span class="pre">test_message</span></code></p>
<p>How would you determine the model’s prediction and its level of certainty?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_message</span> <span class="o">=</span> <span class="s2">&quot;Good. No cellphones allowed :)&quot;</span>
<span class="n">s1</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Thinkin about someone is all good. No drugs for that&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;For sale - arsenal dartboard. Good condition but no doubles or trebles!&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;K:)k:)good:)study well.&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;No management puzzeles.&quot;</span><span class="p">,</span> 
      <span class="s2">&quot;have a good weekend.&quot;</span>
     <span class="p">]</span>
<span class="n">s2</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Five closest neighbours of the message: &quot;</span><span class="p">,</span> <span class="n">test_message</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;Nearest neighbour&quot;</span><span class="p">:</span> <span class="n">s1</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span> <span class="n">s2</span><span class="p">})</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Five closest neighbours of the message:  Good. No cellphones allowed :)
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Nearest neighbour</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Thinkin about someone is all good. No drugs for that</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>1</th>
      <td>For sale - arsenal dartboard. Good condition but no doubles or trebles!</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>2</th>
      <td>K:)k:)good:)study well.</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>3</th>
      <td>No management puzzeles.</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>4</th>
      <td>have a good weekend.</td>
      <td>ham</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><br><br><br><br></p>
</section>
<section id="exercise-7-1-3-calculating-p-spam-mid-message-and-p-ham-mid-message">
<h3>Exercise 7.1.3 Calculating <span class="math notranslate nohighlight">\(P(spam \mid message)\)</span> and <span class="math notranslate nohighlight">\(P(ham \mid message)\)</span><a class="headerlink" href="#exercise-7-1-3-calculating-p-spam-mid-message-and-p-ham-mid-message" title="Permalink to this heading">#</a></h3>
<p>These are reasonable approaches to quantify the level of certainty of models. But how about directly calculating actual probabilities <span class="math notranslate nohighlight">\(P(spam \mid message)\)</span> and <span class="math notranslate nohighlight">\(P(ham \mid message)\)</span> from the training data? What are the challenges associated with directly calculating these probabilities?</p>
<p>Let’s consider a simple example to provide a clearer understanding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_toy</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Lol you are always so convincing. I&#39;m going to give you a free advice. It&#39;s urgent.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Block 2 has interesting courses.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Block 2 has been great so far.&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">y_toy</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">]</span>

<span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;sms&quot;</span><span class="p">:</span><span class="n">X_toy</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span><span class="n">y_toy</span><span class="p">})</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sms</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Lol you are always so convincing. I'm going to give you a free advice. It's urgent.</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Block 2 has interesting courses.</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>3</th>
      <td>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Block 2 has been great so far.</td>
      <td>ham</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Given this training data and a new unseen message “I’m happy to offer some free advice.”, how would you calculate the following probabilities?</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(spam \mid \text{I'm happy to offer some free advice})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(ham \mid \text{I'm happy to offer some free advice})\)</span></p></li>
</ul>
<p>How about using the Bayes rule to calculate these probabilities?</p>
<div class="math notranslate nohighlight">
\[P(A \mid B) = \frac{P(B \mid A) \times P(A)}{P(B)}\]</div>
<div class="math notranslate nohighlight">
\[P(\text{spam} \mid \text{message})= \frac{P(message \mid spam) \times P(spam)}{P(message)}\]</div>
<p><br><br></p>
<div class="math notranslate nohighlight">
\[P(\text{ham} \mid \text{message})= \frac{P(message \mid ham) \times P(ham)}{P(message)}\]</div>
<p>Does it make it easier? In this case, we need the following probabilities.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\text{spam})\)</span> and <span class="math notranslate nohighlight">\(P(\text{ham})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{message})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{message} \mid \text{spam})\)</span> and <span class="math notranslate nohighlight">\(P(\text{message} \mid \text{ham})\)</span></p></li>
</ul>
<section id="the-prior-probabilities-p-text-spam-and-p-text-ham">
<h4>The prior probabilities <span class="math notranslate nohighlight">\(P(\text{spam})\)</span> and <span class="math notranslate nohighlight">\(P(\text{ham})\)</span><a class="headerlink" href="#the-prior-probabilities-p-text-spam-and-p-text-ham" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>marginal probability that a message is spam</p></li>
<li><p>marginal probability that a message is ham</p></li>
</ul>
<p><img alt="" src="../_images/prior_prob.png" /></p>
<!-- <img src='./img/prior_prob1.png' width="500"> --></section>
<section id="p-text-message">
<h4><span class="math notranslate nohighlight">\(P(\text{message})\)</span><a class="headerlink" href="#p-text-message" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>marginal probability of a message</p>
<ul>
<li><p>Hard to calculate but can be ignored in our scenario as it occurs in the denominator for both <span class="math notranslate nohighlight">\(P(\text{spam} \mid \text{message})\)</span> and <span class="math notranslate nohighlight">\(P(\text{ham} \mid \text{message})\)</span>.</p></li>
<li><p>Since our goal is to get good predictions, and we don’t specifically care about the actual probability values, we could ignore the denominator in both cases.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/prob_message.png" /></p>
<!-- <img src='./img/prob_message.png' width="500"> --></section>
<section id="p-text-message-mid-text-spam">
<h4><span class="math notranslate nohighlight">\(P(\text{message} \mid \text{spam})\)</span><a class="headerlink" href="#p-text-message-mid-text-spam" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The conditional probability <span class="math notranslate nohighlight">\(P(\text{message}\mid\text{spam})\)</span> is hard to calculate because it requires a huge number of parameters and impossibly large training sets.</p></li>
<li><p>In our toy example:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(\text{message}\mid\text{spam}) = \)</span><span class="math notranslate nohighlight">\(P(\text{I} \mid spam) \times P(\text{am} \mid spam, \text{I}) \times \dots \times P(\text{advice} \mid spam, \text{I am happy to offer some free})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{message}\mid\text{ham}) = \)</span><span class="math notranslate nohighlight">\(P(\text{I} \mid ham) \times P(\text{am} \mid ham, \text{I}) \times \dots \times P(\text{advice} \mid ham, \text{I am happy to offer some free})\)</span></p></li>
</ul>
</li>
<li><p>with large number of features, it would require huge numbers of parameters and impossibly large training sets.</p></li>
<li><p>For example, imagine how hard it would be to estimate probability of the 5,000th word, given the previous 4,999 words.</p></li>
</ul>
<p>…But we need this probability. So we start making assumptions.</p>
<p><strong>Naive Bayes</strong>, the topic of this lecture makes two simplifying assumptions:</p>
<p><strong>Bag-of-words representation</strong></p>
<p>Information about the order and structure of words in the text is lost, but the frequency or presence of each word is retained.</p>
<p><strong>Conditional independence assumption</strong></p>
<p>This is the “naive” part of naive Bayes. Given the target (like spam or not-spam), the presence (or absence) of each word is assumed to be <strong>conditionally independent</strong> of the presence (or absence) of any other word. This assumption simplifies the computation and makes the method tractable for high-dimensional data, even though it’s a strong and often unrealistic assumption.</p>
<div class="math notranslate nohighlight">
\[P(\text{I am happy to offer some free advice}\mid\text{spam}) \approx P(\text{I} \mid spam) \times P(\text{am} \mid spam) \times P(\text{happy} \mid spam) \times \dots \times P(\text{advice} \mid spam)\]</div>
<p><br><br><br><br></p>
</section>
</section>
</section>
<section id="naive-bayes">
<h2>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Our first probabilistic classifier where we think of learning as a problem of statistical inference.</p></li>
<li><p>In late 1990’s a graduate student from Stanford, Mehran Sahami, did an internship at Microsoft Research, where he collaborated with others on the use of Naive Bayes for spam filtering. The work was pioneering at that time.</p>
<ul>
<li><p>Check out <a class="reference external" href="http://robotics.stanford.edu/users/sahami/papers-dir/spam.pdf">the original paper</a></p></li>
</ul>
</li>
<li><p>For years, best spam filtering methods used naive Bayes.</p></li>
<li><p>For example, <a class="reference external" href="https://spamassassin.apache.org/">SpamAssassin</a> is a spam filtering system based on naive Bayes.</p></li>
<li><p>Even today naive Bayes forms a basis for many disease diagnosis and spam filtering systems.</p></li>
<li><p>At some point it was one of the most widely used learner at Google.</p></li>
</ul>
<section id="svc-classifier-for-spam-detection">
<h3>SVC classifier for spam detection<a class="headerlink" href="#svc-classifier-for-spam-detection" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Before discussing how it works, let’s try it out with sklearn.</p></li>
<li><p>Let’s first try SVC classifier on <a class="reference external" href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset">the SMS Spam Collection Dataset</a>. (This is what you did in the lab last week.)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sms_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/spam.csv&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;latin-1&quot;</span><span class="p">)</span>
<span class="n">sms_df</span> <span class="o">=</span> <span class="n">sms_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">sms_df</span> <span class="o">=</span> <span class="n">sms_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;v1&quot;</span><span class="p">:</span> <span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="s2">&quot;v2&quot;</span><span class="p">:</span> <span class="s2">&quot;sms&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">FileNotFoundError</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="n">sms_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/spam.csv&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;latin-1&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">sms_df</span> <span class="o">=</span> <span class="n">sms_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;Unnamed: 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 3&quot;</span><span class="p">,</span> <span class="s2">&quot;Unnamed: 4&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">sms_df</span> <span class="o">=</span> <span class="n">sms_df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;v1&quot;</span><span class="p">:</span> <span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="s2">&quot;v2&quot;</span><span class="p">:</span> <span class="s2">&quot;sms&quot;</span><span class="p">})</span>

<span class="nn">File ~/miniconda3/envs/571/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948,</span> in <span class="ni">read_csv</span><span class="nt">(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)</span>
<span class="g g-Whitespace">    </span><span class="mi">935</span> <span class="n">kwds_defaults</span> <span class="o">=</span> <span class="n">_refine_defaults_read</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">936</span>     <span class="n">dialect</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">937</span>     <span class="n">delimiter</span><span class="p">,</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">944</span>     <span class="n">dtype_backend</span><span class="o">=</span><span class="n">dtype_backend</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">945</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">946</span> <span class="n">kwds</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwds_defaults</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">948</span> <span class="k">return</span> <span class="n">_read</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="n">kwds</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/envs/571/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611,</span> in <span class="ni">_read</span><span class="nt">(filepath_or_buffer, kwds)</span>
<span class="g g-Whitespace">    </span><span class="mi">608</span> <span class="n">_validate_names</span><span class="p">(</span><span class="n">kwds</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;names&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">610</span> <span class="c1"># Create the parser.</span>
<span class="ne">--&gt; </span><span class="mi">611</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">TextFileReader</span><span class="p">(</span><span class="n">filepath_or_buffer</span><span class="p">,</span> <span class="o">**</span><span class="n">kwds</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">613</span> <span class="k">if</span> <span class="n">chunksize</span> <span class="ow">or</span> <span class="n">iterator</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">614</span>     <span class="k">return</span> <span class="n">parser</span>

<span class="nn">File ~/miniconda3/envs/571/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448,</span> in <span class="ni">TextFileReader.__init__</span><span class="nt">(self, f, engine, **kwds)</span>
<span class="g g-Whitespace">   </span><span class="mi">1445</span>     <span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kwds</span><span class="p">[</span><span class="s2">&quot;has_index_names&quot;</span><span class="p">]</span>
<span class="g g-Whitespace">   </span><span class="mi">1447</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="p">:</span> <span class="n">IOHandles</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1448</span> <span class="bp">self</span><span class="o">.</span><span class="n">_engine</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_engine</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="p">)</span>

<span class="nn">File ~/miniconda3/envs/571/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705,</span> in <span class="ni">TextFileReader._make_engine</span><span class="nt">(self, f, engine)</span>
<span class="g g-Whitespace">   </span><span class="mi">1703</span>     <span class="k">if</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1704</span>         <span class="n">mode</span> <span class="o">+=</span> <span class="s2">&quot;b&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1705</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="o">=</span> <span class="n">get_handle</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1706</span>     <span class="n">f</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1707</span>     <span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1708</span>     <span class="n">encoding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1709</span>     <span class="n">compression</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;compression&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1710</span>     <span class="n">memory_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;memory_map&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1711</span>     <span class="n">is_text</span><span class="o">=</span><span class="n">is_text</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1712</span>     <span class="n">errors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;encoding_errors&quot;</span><span class="p">,</span> <span class="s2">&quot;strict&quot;</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1713</span>     <span class="n">storage_options</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">options</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;storage_options&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="g g-Whitespace">   </span><span class="mi">1714</span> <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1715</span> <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1716</span> <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">handles</span><span class="o">.</span><span class="n">handle</span>

<span class="nn">File ~/miniconda3/envs/571/lib/python3.10/site-packages/pandas/io/common.py:863,</span> in <span class="ni">get_handle</span><span class="nt">(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)</span>
<span class="g g-Whitespace">    </span><span class="mi">858</span> <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">859</span>     <span class="c1"># Check whether the filename is to be opened in binary mode.</span>
<span class="g g-Whitespace">    </span><span class="mi">860</span>     <span class="c1"># Binary mode does not support &#39;encoding&#39; and &#39;newline&#39;.</span>
<span class="g g-Whitespace">    </span><span class="mi">861</span>     <span class="k">if</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span> <span class="ow">and</span> <span class="s2">&quot;b&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">862</span>         <span class="c1"># Encoding</span>
<span class="ne">--&gt; </span><span class="mi">863</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">864</span>             <span class="n">handle</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">865</span>             <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">866</span>             <span class="n">encoding</span><span class="o">=</span><span class="n">ioargs</span><span class="o">.</span><span class="n">encoding</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">867</span>             <span class="n">errors</span><span class="o">=</span><span class="n">errors</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">868</span>             <span class="n">newline</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">869</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">870</span>     <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">871</span>         <span class="c1"># Binary mode</span>
<span class="g g-Whitespace">    </span><span class="mi">872</span>         <span class="n">handle</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">handle</span><span class="p">,</span> <span class="n">ioargs</span><span class="o">.</span><span class="n">mode</span><span class="p">)</span>

<span class="ne">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;data/spam.csv&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">sms_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;sms&quot;</span><span class="p">],</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;sms&quot;</span><span class="p">],</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>sms</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>385</th>
      <td>ham</td>
      <td>It took Mr owl 3 licks</td>
    </tr>
    <tr>
      <th>4003</th>
      <td>ham</td>
      <td>Well there's a pattern emerging of my friends telling me to drive up and come smoke with them and then telling me that I'm a weed fiend/make them smoke too much/impede their doing other things so ...</td>
    </tr>
    <tr>
      <th>1283</th>
      <td>ham</td>
      <td>Yes i thought so. Thanks.</td>
    </tr>
    <tr>
      <th>2327</th>
      <td>spam</td>
      <td>URGENT! Your mobile number *************** WON a å£2000 Bonus Caller prize on 10/06/03! This is the 2nd attempt to reach you! Call 09066368753 ASAP! Box 97N7QP, 150ppm</td>
    </tr>
    <tr>
      <th>1103</th>
      <td>ham</td>
      <td>Aiyah sorry lor... I watch tv watch until i forgot 2 check my phone.</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">pipe_svc</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">())</span>
<span class="n">results_dict</span><span class="p">[</span><span class="s2">&quot;SVC&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_svc</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/MDS/2023-24/571/DSCI_571_sup-learn-1_students/lectures/code/utils.py:86: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  out_col.append((f&quot;%0.3f (+/- %0.3f)&quot; % (mean_scores[i], std_scores[i])))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SVC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.483 (+/- 0.017)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.113 (+/- 0.006)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.978 (+/- 0.005)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.995 (+/- 0.001)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="sklearn-naive-bayes-classifier">
<h3><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> naive Bayes classifier<a class="headerlink" href="#sklearn-naive-bayes-classifier" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Let’s try Naive Bayes on this problem.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span><span class="p">,</span> <span class="n">BernoulliNB</span>

<span class="n">pipe_nb</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span> <span class="n">MultinomialNB</span><span class="p">())</span>
<span class="n">results_dict</span><span class="p">[</span><span class="s2">&quot;Naive Bayes&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_std_cross_val_scores</span><span class="p">(</span>
    <span class="n">pipe_nb</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/MDS/2023-24/571/DSCI_571_sup-learn-1_students/lectures/code/utils.py:86: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  out_col.append((f&quot;%0.3f (+/- %0.3f)&quot; % (mean_scores[i], std_scores[i])))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_dict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>SVC</th>
      <th>Naive Bayes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fit_time</th>
      <td>0.483 (+/- 0.017)</td>
      <td>0.041 (+/- 0.001)</td>
    </tr>
    <tr>
      <th>score_time</th>
      <td>0.113 (+/- 0.006)</td>
      <td>0.009 (+/- 0.000)</td>
    </tr>
    <tr>
      <th>test_score</th>
      <td>0.978 (+/- 0.005)</td>
      <td>0.986 (+/- 0.003)</td>
    </tr>
    <tr>
      <th>train_score</th>
      <td>0.995 (+/- 0.001)</td>
      <td>0.993 (+/- 0.001)</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The validation scores are a bit better on this particular dataset.</p></li>
<li><p>Much faster than the SVC classifier!!</p></li>
</ul>
<p>Now let’s try to understand how <code class="docutils literal notranslate"><span class="pre">predict</span></code> and <code class="docutils literal notranslate"><span class="pre">fit</span></code> work for this model. Let’s bring back the toy dataset from before and encode it with BOW representation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_toy</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Lol you are always so convincing. I&#39;m going to give you a free advice. It&#39;s urgent.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Block 2 has interesting courses.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Block 2 has been great so far.&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">y_toy</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">]</span>
<span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s2">&quot;sms&quot;</span><span class="p">:</span><span class="n">X_toy</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">:</span><span class="n">y_toy</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>For simplicity, let’s use <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> with <code class="docutils literal notranslate"><span class="pre">binary=True</span></code>, i.e., assume presence or absence of words instead of counts and only consider the top 4 features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_toy_vec</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_toy</span><span class="p">,</span> <span class="n">y_toy</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_bow_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">X_toy_vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">X_toy</span>
<span class="p">)</span>
<span class="n">toy_bow_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_toy</span>
<span class="n">toy_bow_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>block</th>
      <th>free</th>
      <th>prize</th>
      <th>urgent</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Lol you are always so convincing. I'm going to give you a free advice. It's urgent.</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>Block 2 has interesting courses.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Block 2 has been great so far.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="naive-bayes-predict">
<h3>Naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code><a class="headerlink" href="#naive-bayes-predict" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Given new messages, we want to predict whether they are spam or ham.</p></li>
<li><p>Example: Predict whether the following message is spam or ham.</p></li>
</ul>
<blockquote>
<div><p>“URGENT! Free!!”</p>
</div></blockquote>
<p>Remember the two simplifying assumptions of naive Bayes?</p>
<p><strong>BOW representaion</strong>: First, let’s get BOW representation of the message.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">deploy_test</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;URGENT! Free!!&quot;</span><span class="p">,</span> <span class="s2">&quot;Let&#39;s enjoy the last week of block 2!&quot;</span><span class="p">]</span>
<span class="n">deploy_bow</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">deploy_test</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">bow_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">deploy_bow</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">deploy_test</span>
<span class="p">)</span>
<span class="n">bow_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>block</th>
      <th>free</th>
      <th>prize</th>
      <th>urgent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>URGENT! Free!!</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>Let's enjoy the last week of block 2!</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s try predicting with <code class="docutils literal notranslate"><span class="pre">pipe_nb</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">pipe_nb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">deploy_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;spam&#39;, &#39;ham&#39;], dtype=&#39;&lt;U4&#39;)
</pre></div>
</div>
</div>
</div>
<p>What’s happening under the hood? To predict the correct class, assuming <strong>conditional independence</strong>, naive Bayes estimates some scores which are proportional to the following probabilities and picks the target with higher score.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\text{spam} \mid \text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent=1}) \propto  P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent=1} \mid \text{spam}) \times P(spam)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{ham} \mid  \text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent=1}) \propto  P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent=1} \mid \text{spam}) \times P(ham)\)</span></p></li>
</ul>
<p>For each target, we need to calculate two probabilities:</p>
<ul class="simple">
<li><p><strong>Prior probabilities</strong>: <span class="math notranslate nohighlight">\(P(\text{target})\)</span></p></li>
<li><p><strong>Conditional probabilities</strong>: <span class="math notranslate nohighlight">\(P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent=1} \mid \text{target})\)</span></p></li>
</ul>
<section id="computing-prior-probabilities">
<h4>Computing prior probabilities<a class="headerlink" href="#computing-prior-probabilities" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Prior probability: what’s the proportion of the target class occurs in our training dataset?</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(\text{spam}) = 3/6\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{ham}) = 3/6\)</span></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_bow_df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>target
spam    3
ham     3
Name: count, dtype: int64
</pre></div>
</div>
</div>
</div>
</section>
<section id="computing-conditional-probabilities">
<h4>Computing conditional probabilities<a class="headerlink" href="#computing-conditional-probabilities" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>Assumption: <strong>Features are independent, conditioned on the target</strong>.</p>
<ul>
<li><p>Example: In our spam classification example, <strong>once you know that a message is spam</strong>, the probability that the word “urgent” appears is independent of whether “free” also appeared.</p></li>
<li><p>So if you know that a message is spam, knowing whether the word “urgent” occurs gives no additional information about whether “free” also appears.</p></li>
</ul>
</li>
<li><p>We can write this mathematically as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
&amp; P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent} = 1 \mid \text{spam}) \\
&amp;\approx P(\text{block} = 0 \mid \text{spam}) \times P(\text{free} = 1 \mid \text{spam}) \times P(\text{prize} = 0 \mid \text{spam}) \times P(\text{urgent} = 1 \mid \text{spam})\\\\
&amp; P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent} = 1 \mid \text{ham}) \\
&amp;\approx P(\text{block} = 0 \mid \text{ham}) \times P(\text{free} = 1 \mid \text{ham}) \times P(\text{prize} = 0 \mid \text{ham}) \times P(\text{urgent} = 1 \mid \text{ham})
\end{split}
\end{equation}\end{split}\]</div>
<p>We can calculate these based on the frequencies of these words in our training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_toy_df</span> <span class="o">=</span> <span class="n">toy_df</span><span class="p">[</span><span class="n">toy_df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;spam&#39;</span><span class="p">]</span>
<span class="n">ham_toy_df</span> <span class="o">=</span> <span class="n">toy_df</span><span class="p">[</span><span class="n">toy_df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;ham&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_spam_ham_word_freq</span><span class="p">(</span><span class="n">spam_toy_df</span><span class="p">[</span><span class="s1">&#39;sms&#39;</span><span class="p">],</span> <span class="n">ham_toy_df</span><span class="p">[</span><span class="s1">&#39;sms&#39;</span><span class="p">])</span> <span class="c1"># custom function defined in code/plotting_functions.oy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/MDS/2023-24/571/DSCI_571_sup-learn-1_students/lectures/code/plotting_functions.py:99: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax[0].set_xticklabels(labels=spam_words, rotation=45)
/Users/kvarada/MDS/2023-24/571/DSCI_571_sup-learn-1_students/lectures/code/plotting_functions.py:108: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.
  ax[1].set_xticklabels(labels=ham_words, rotation=45)
</pre></div>
</div>
<img alt="../_images/ca4b5e30e0e488ee0cb001f316153e64d0a5e04f1f9c6e7abc7a5ee104625eee.png" src="../_images/ca4b5e30e0e488ee0cb001f316153e64d0a5e04f1f9c6e7abc7a5ee104625eee.png" />
</div>
</div>
<p>So in this toy example, for each target, we need the following conditional probabilities for prediction.</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(P(\text{block} = 0 \mid \text{target})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{free} = 1 \mid \text{target})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{prize} = 0 \mid \text{target})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{urgent} = 1 \mid \text{target})\)</span>
use our training data to estimate these probabilities.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_bow_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>block</th>
      <th>free</th>
      <th>prize</th>
      <th>urgent</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Lol you are always so convincing. I'm going to give you a free advice. It's urgent.</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>Block 2 has interesting courses.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Block 2 has been great so far.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="conditional-probabilities-for-the-target-spam">
<h4>Conditional probabilities for the target spam<a class="headerlink" href="#conditional-probabilities-for-the-target-spam" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>What is <span class="math notranslate nohighlight">\(P(\text{block} = 0 \mid \text{spam})\)</span>?</p>
<ul>
<li><p>Given target is spam, how often “block” = 0? <span class="math notranslate nohighlight">\(3/3\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(P(\text{free} = 1 \mid \text{spam}) = 2/3\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{prize} = 0 \mid \text{spam}) = 1/3\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{urgent} = 1 \mid \text{spam}) = 2/3\)</span></p></li>
</ul>
<p>So using naive Bayes, <span class="math notranslate nohighlight">\(P(\text{spam} \mid \text{message})\)</span> is proportional to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
\begin{split}
P(\text{spam} \mid \text{message}) &amp;\propto P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent} = 1 \mid \text{spam}) \times P(\text{spam})\\
&amp;\propto P(\text{block} = 0 \mid \text{spam}) \times P(\text{free} = 1 \mid \text{spam}) \\
&amp; \times P(\text{prize} = 0 \mid \text{spam}) \times P(\text{urgent} = 1 \mid \text{spam}) \times P(\text{spam})\\
&amp;\propto 3/3 \times 2/3 \times 1/3 \times 2/3 \times 3/6\\
\end{split}
\end{equation}\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_prior</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">6</span>
<span class="n">block0_spam</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">free1_spam</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">prize0_spam</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">urgent1_spam</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">spam_estimate</span> <span class="o">=</span> <span class="n">spam_prior</span> <span class="o">*</span> <span class="n">block0_spam</span> <span class="o">*</span> <span class="n">free1_spam</span> <span class="o">*</span> <span class="n">prize0_spam</span> <span class="o">*</span> <span class="n">urgent1_spam</span>
<span class="n">spam_estimate</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.07407407407407407
</pre></div>
</div>
</div>
</div>
</section>
<section id="conditional-probabilities-for-the-target-ham">
<h4>Conditional probabilities for the target ham<a class="headerlink" href="#conditional-probabilities-for-the-target-ham" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>What is <span class="math notranslate nohighlight">\(P(\text{block} = 0 \mid \text{ham})\)</span>?</p>
<ul>
<li><p>Given target is ham, how often “block” = 0? <span class="math notranslate nohighlight">\(1/3\)</span></p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(P(\text{free} = 1 \mid \text{ham}) = 1/3\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{prize} = 0 \mid \text{ham}) = 3/3\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{urgent} = 1 \mid \text{ham}) = 1/3\)</span></p></li>
</ul>
<p>So using naive Bayes, <span class="math notranslate nohighlight">\(P(\text{ham} \mid \text{message})\)</span> is proportional to
$<span class="math notranslate nohighlight">\(\begin{equation}
\begin{split}
P(\text{ham} \mid \text{message}) &amp;\propto P(\text{block} = 0, \text{free} = 1, \text{prize} = 0, \text{urgent} = 1 \mid \text{ham}) \times P(\text{ham})\\
&amp;\propto P(\text{block} = 0 \mid \text{ham}) \times P(\text{free} = 1 \mid \text{ham}) \\
&amp; \times P(\text{prize} = 0 \mid \text{ham}) \times P(\text{urgent} = 1 \mid \text{ham}) \times P(\text{ham})\\
&amp;\propto 1/3 \times 1/3 \times 3/3 \times 1/3 \times 3/6\\
\end{split}
\end{equation}\)</span>$</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ham_prior</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">6</span>
<span class="n">block0_ham</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">free1_ham</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">prize0_ham</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">urgent1_ham</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">ham_estimate</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">ham_prior</span>
    <span class="o">*</span> <span class="n">block0_ham</span>
    <span class="o">*</span> <span class="n">free1_ham</span>
    <span class="o">*</span> <span class="n">prize0_ham</span>
    <span class="o">*</span> <span class="n">urgent1_ham</span>
<span class="p">)</span>
<span class="n">ham_estimate</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.018518518518518517
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;spam estimate: &#39;</span><span class="p">,</span> <span class="n">spam_estimate</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ham estimate: &#39;</span><span class="p">,</span> <span class="n">ham_estimate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>spam estimate:  0.07407407407407407
ham estimate:  0.018518518518518517
</pre></div>
</div>
</div>
</div>
<p>Since <span class="math notranslate nohighlight">\(P(spam \mid message)\)</span> is proportional to a bigger number, we pick “spam” as the label.</p>
<ul class="simple">
<li><p>Note that these estimates are not well-defined probabilities, as we ignored the denominator when calculating these estimates.</p></li>
<li><p>Let’s normalize these estimates so that they sum to 1.0</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">spam_estimate</span> <span class="o">+</span> <span class="n">ham_estimate</span>
<span class="n">spam_prob_score</span> <span class="o">=</span> <span class="n">spam_estimate</span> <span class="o">/</span> <span class="n">normalizer</span>
<span class="n">ham_prob_score</span> <span class="o">=</span> <span class="n">ham_estimate</span> <span class="o">/</span> <span class="n">normalizer</span>
<span class="p">[</span><span class="n">ham_prob_score</span><span class="p">,</span> <span class="n">spam_prob_score</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.2, 0.8]
</pre></div>
</div>
</div>
</div>
<p>Let’s examine whether we get the same numbers with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code>.</p>
<ul class="simple">
<li><p>We are using <code class="docutils literal notranslate"><span class="pre">BernoulliNB</span></code> because our features are binary.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_toy_vec</span><span class="p">,</span> <span class="n">y_toy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages/sklearn/naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.
  warnings.warn(
/Users/kvarada/miniconda3/envs/571/lib/python3.10/site-packages/sklearn/naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.
  warnings.warn(
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>BernoulliNB(alpha=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">BernoulliNB</label><div class="sk-toggleable__content"><pre>BernoulliNB(alpha=0)</pre></div></div></div></div></div></div></div>
</div>
<ul class="simple">
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method to get the estimated probability scores.</p></li>
<li><p>More on <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> in the next class.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">deploy_bow</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.2, 0.8])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;ham&#39;, &#39;spam&#39;], dtype=&#39;&lt;U4&#39;)
</pre></div>
</div>
</div>
</div>
<p>They match with what we calculated by hand 🎉🎉!!</p>
<p><br><br></p>
</section>
</section>
<section id="summary-of-naive-bayes-predict">
<h3>Summary of naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code><a class="headerlink" href="#summary-of-naive-bayes-predict" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Use Bayes rule to calculate the conditional probability of each target given the message.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(\text{target} \mid \text{message}) = \frac{P(\text{message} \mid \text{target}) \times P(\text{target})}{P(\text{message})} \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(\text{message})\)</span>: marginal probability that a message has the given set of words</p>
<ul>
<li><p>Hard to calculate but can be ignored in our scenario.</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ P(\text{target} \mid \text{message}) \propto P(\text{message} \mid\text{target}) \times P(\text{target}) =  P(w_1, w_2, \dots, w_d \mid \text{target}) \times P(\text{target})\]</div>
<ul>
<li><p>We need the following terms:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P(\text{target})\)</span>: marginal probability that a message has the class target</p></li>
<li><p><span class="math notranslate nohighlight">\(P(\text{message}\mid\text{target})\)</span>: conditional probability that message has words <span class="math notranslate nohighlight">\(w_1, w_2, \dots, w_d\)</span>, given that it is of the given target.</p>
<ul class="simple">
<li><p>Hard to calculate but we need it. We would require huge numbers of parameters and impossibly large training sets.</p></li>
<li><p>Naive assumption that features are independent of each other conditioned on the target.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(\text{message} \mid \text{target}) = P(w_1, w_2, . . . , w_d \mid \text{target}) \approx \prod_{i=1}^{d}P(w_i \mid \text{target})\]</div>
</li>
</ul>
</li>
</ul>
</section>
<section id="summary-naive-bayes-predict">
<h3>Summary naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code><a class="headerlink" href="#summary-naive-bayes-predict" title="Permalink to this heading">#</a></h3>
<p>Using naive Bayes assumption, estimate scores that are proportional to the following probabilities.</p>
<div class="math notranslate nohighlight">
\[ P(\text{target} \mid \text{message}) \propto P(\text{target}) \times  \prod_{i=1}^{d}P(w_i \mid \text{target})\]</div>
<ul class="simple">
<li><p>Predict the target where <span class="math notranslate nohighlight">\(P(\text{target} \mid \text{message})\)</span> is proportional to a bigger value.</p></li>
</ul>
</section>
<section id="id1">
<h3>❓❓ Questions for you<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>When we ignore the marginal probability P(message) when calculating <span class="math notranslate nohighlight">\(P(\text{spam} | \text{message})\)</span> or <span class="math notranslate nohighlight">\(P(\text{ham} | \text{message})\)</span>, are these going to be well-defined probabilites? Does it matter?</p></li>
<li><p>When calculating conditional probabilities, why do we consider both present and absent words?</p></li>
</ul>
</section>
</section>
<section id="id2">
<h2>❓❓ Questions for you<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<section id="iclicker-exercise-7-2">
<h3>(iClicker) Exercise 7.2<a class="headerlink" href="#iclicker-exercise-7-2" title="Permalink to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/DAZZ</strong></p>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) In Bernoulli naive Bayes, for binary classification, we need to calculate <span class="math notranslate nohighlight">\(2d+1\)</span> probabilities, where <span class="math notranslate nohighlight">\(d\)</span> is the number of unique words.</p></li>
<li><p>(B) Naive Bayes should only be used if the “naive” conditional independence assumption holds for your problem.</p></li>
<li><p>(C) Naive Bayes should only be used when the features (i.e. <code class="docutils literal notranslate"><span class="pre">X</span></code>-values) are binary.`</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">V’s Solutions!</p>
<ul class="simple">
<li><p>A</p></li>
</ul>
</div>
</section>
</section>
<section id="break-5-min">
<h2>Break (5 min)<a class="headerlink" href="#break-5-min" title="Permalink to this heading">#</a></h2>
<p><img alt="" src="../_images/eva-coffee.png" /></p>
<section id="naive-bayes-classifier-fit">
<h3>Naive Bayes classifier <code class="docutils literal notranslate"><span class="pre">fit</span></code><a class="headerlink" href="#naive-bayes-classifier-fit" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Counting and estimating probabilities.</p></li>
<li><p>Estimate prior probabilities and conditional probabilities for each feature given each class.</p></li>
</ul>
</section>
<section id="prior-probabilities">
<h3>Prior probabilities<a class="headerlink" href="#prior-probabilities" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>P(target) <span class="math notranslate nohighlight">\(\rightarrow\)</span> how often target <span class="math notranslate nohighlight">\(c\)</span> occurs in our dataset in comparison with all other targets.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(c) = \frac{N_c}{N}\]</div>
<p>where,</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_c\)</span> is the number of examples with target <span class="math notranslate nohighlight">\(c\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is total number of examples</p></li>
</ul>
</section>
<section id="conditional-probabilities">
<h3>Conditional probabilities<a class="headerlink" href="#conditional-probabilities" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Above, we only calculated conditional probabilities which were required for estimating probability score for class given given message.</p></li>
<li><p>Naive Bayes calculates all conditional probabilities for each feature given each class in advance during <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
<li><p>Then during <code class="docutils literal notranslate"><span class="pre">predict</span></code> it just multiplies the appropriate probabilities.</p></li>
</ul>
<p>In case of Bernoulli naive Bayes with binary classification, for each feature <span class="math notranslate nohighlight">\(w\)</span> and target <span class="math notranslate nohighlight">\(c\)</span>, it calculates four probabilities:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(w = 1 \mid c = 0) \rightarrow\)</span> Probability that the word is present given target 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(w = 0 \mid c = 0) \rightarrow\)</span> Probability that the word is absent given target 0.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(w = 1 \mid c = 1) \rightarrow\)</span> Probability that the word is present given target 1.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(w = 0 \mid c = 1) \rightarrow\)</span> Probability that the word is absent given target 1.</p></li>
</ul>
<p>But <span class="math notranslate nohighlight">\(P(w = 0 \mid c = 0) = 1 - P(w = 1 \mid c = 0)\)</span>
And <span class="math notranslate nohighlight">\(P(w = 0 \mid c = 1) = 1 - P(w = 1 \mid c = 1)\)</span></p>
<p>So in reality we just need to calculate two probabilities for each word.</p>
<p>We calculate these probabilities by counting word and target occurrences in the training data. For example,
$<span class="math notranslate nohighlight">\( P(w = 1\mid c = 1) = \frac{Count(w = 1, c = 1)}{Count(c = 1)}\)</span>$</p>
<ul class="simple">
<li><p>This is called <em>maximum likelihood estimation</em>.</p></li>
<li><p>It stores these probabilities, which will be later used during predict time.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="laplace-smoothing">
<h2>Laplace smoothing<a class="headerlink" href="#laplace-smoothing" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_bow_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>block</th>
      <th>free</th>
      <th>prize</th>
      <th>urgent</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>URGENT!! As a valued network customer you have been selected to receive a £900 prize reward!</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Lol you are always so convincing. I'm going to give you a free advice. It's urgent.</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>Block 2 has interesting courses.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
    <tr>
      <th>URGENT! You have won a 1 week FREE membership in our £100000 prize Jackpot!</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free!</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>spam</td>
    </tr>
    <tr>
      <th>Block 2 has been great so far.</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>ham</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What will happen if you want to predict for:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_ex</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;urgent free block&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformed</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_ex</span><span class="p">)</span>
<span class="n">transformed</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 1, 0, 1]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_prior</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">6</span>
<span class="n">block1_spam</span> <span class="o">=</span> <span class="mi">0</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">free1_spam</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">prize0_spam</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span>
<span class="n">urgent1_spam</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_prior</span> <span class="o">*</span> <span class="n">block1_spam</span> <span class="o">*</span> <span class="n">free1_spam</span> <span class="o">*</span> <span class="n">prize0_spam</span> <span class="o">*</span> <span class="n">urgent1_spam</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Although all the other words are yelling SPAM! SPAM! SPAM!, since we do not have any example where the word “block” occurs with spam, we get a zero probability.</p></li>
</ul>
<p><img alt="" src="../_images/eva-qm.png" /></p>
<ul class="simple">
<li><p>Naive Bayes naively multiplies all the feature likelihoods together, and if any of the terms is zero, it’s going to void all other evidence and the probability of the class is going to be zero.</p></li>
<li><p>Sounds worrisome!</p></li>
<li><p>We have limited data and if we do not see a feature occurring with a class, it doesn’t mean it would never occur with that class.</p></li>
<li><p>How can we tackle this and avoid obliterating all other probabilities because of one zero in the product?</p></li>
</ul>
<section id="a-simplest-solution-laplace-smoothing">
<h3>A simplest solution: Laplace smoothing<a class="headerlink" href="#a-simplest-solution-laplace-smoothing" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The simplest way to avoid zero probabilities is to add one to the counts when we calculate conditional probabilities.</p></li>
<li><p>For example, a smoothed probability of<br />
$<span class="math notranslate nohighlight">\( P(\text{block} = 1 \mid \text{spam}) = = \frac{\text{\# documents where block occurs with spam} + 1 }{\text{how often spam occurs} + \text{\# number of possible word values}}\)</span>$</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(\text{block} = 1 \mid \text{spam}) = \frac{0 + 1}{3+2}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_prior</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">/</span> <span class="mi">6</span>
<span class="n">block1_spam_sm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">free1_spam_sm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">prize0_spam_sm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">urgent1_spam_sm</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">spam_prior</span> <span class="o">*</span> <span class="n">block1_spam_sm</span> <span class="o">*</span> <span class="n">free1_spam_sm</span> <span class="o">*</span> <span class="n">prize0_spam_sm</span> <span class="o">*</span> <span class="n">urgent1_spam_sm</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0144
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>All the counts that used to be zero will now have a count of 1</p></li>
</ul>
<ul class="simple">
<li><p>Even though our encoding contains only 0’s and 1’s, we can try different values for smoothing.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> we control it using hyperparameter <code class="docutils literal notranslate"><span class="pre">alpha</span></code> (by default <code class="docutils literal notranslate"><span class="pre">alpha=1.0</span></code>).</p></li>
<li><p>Check out sklearn’s documentation on <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html">BernoulliNB</a></p></li>
</ul>
<p>Let’s try this out on the example above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_ex</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;urgent free block&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transformed</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 1, 0, 1]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We are now getting more sensible probabilities.   </span>
<span class="n">nb_sm</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">nb_sm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_toy_vec</span><span class="p">,</span> <span class="n">y_toy</span><span class="p">)</span>
<span class="n">nb_sm</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">transformed</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.72727273, 0.27272727]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nb_sm</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;ham&#39;, &#39;spam&#39;], dtype=&#39;&lt;U4&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="alpha-hyperparameter-and-the-fundamental-tradeoff">
<h3><code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter and the fundamental tradeoff<a class="headerlink" href="#alpha-hyperparameter-and-the-fundamental-tradeoff" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>High alpha <span class="math notranslate nohighlight">\(\rightarrow\)</span> underfitting</p>
<ul>
<li><p>means we are adding large counts to everything and so we are diluting the data</p></li>
</ul>
</li>
<li><p>Low alpha <span class="math notranslate nohighlight">\(\rightarrow\)</span> overfitting</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;mean_train_score&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;mean_cv_score&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;std_cv_score&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;std_train_score&quot;</span><span class="p">:</span> <span class="p">[],</span>
<span class="p">}</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;bernoullinb__alpha&quot;</span><span class="p">:</span> <span class="mf">10.0</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>

<span class="n">pipe_nb</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span> <span class="n">BernoulliNB</span><span class="p">())</span>
<span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">pipe_nb</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_scores</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_test_score&quot;</span><span class="p">]</span>
<span class="n">train_scores</span> <span class="o">=</span> <span class="n">search</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="s2">&quot;mean_train_score&quot;</span><span class="p">]</span>
<span class="n">alpha_vals</span> <span class="o">=</span> <span class="n">param_grid</span><span class="p">[</span><span class="s2">&quot;bernoullinb__alpha&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alpha_vals</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogx</span><span class="p">(</span><span class="n">alpha_vals</span><span class="p">,</span> <span class="n">test_scores</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;cv&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;laplace smoothing (alpha)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/849f5b93171d2f7c2e4a5099bc61fad94dbd4a334c546f1344ad95c7e80be9d7.png" src="../_images/849f5b93171d2f7c2e4a5099bc61fad94dbd4a334c546f1344ad95c7e80be9d7.png" />
</div>
</div>
</section>
<section id="optional-details-on-the-formulas">
<h3>(Optional) Details on the formulas<a class="headerlink" href="#optional-details-on-the-formulas" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>In case of <code class="docutils literal notranslate"><span class="pre">MultinomialNB</span></code>, the conditional probabilities will change with the following formula.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(w \mid c) = \frac{Count(w, c) + 1 }{Count(c) + \mid vocabulary \mid}\]</div>
<ul class="simple">
<li><p>In case of Bernoulli naive Bayes, when you calculate conditional probabilities with Laplace smoothing, the formula is slightly different:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(w \mid c) = \frac{Count(w, c) + 1 }{Count(c) + 2 }\]</div>
<ul class="simple">
<li><p>So instead of the <span class="math notranslate nohighlight">\(\mid vocabulary \mid\)</span> we just add 2. See the sklearn code <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/naive_bayes.py#L1031">here</a> and <a class="reference external" href="https://stackoverflow.com/questions/40448784/laplace-smoothing-for-bernoulli-model-for-naive-bayes-classifier">this Stack overflow post</a> for explanation.</p></li>
</ul>
<p>Note that when we estimated probabilities in our toy example (e.g., <span class="math notranslate nohighlight">\(P(\text{word} \mid spam)\)</span>), we happened to have each feature value as either 0 or 1, i.e., just the existence of a word in the document’s bag of words. We computed <span class="math notranslate nohighlight">\(P(\text{word} \mid spam)\)</span> as a fraction of times the word appears among all words in all messages of the spam class. If we want to work with frequencies instead of existence, we first concatenate all documents with that class (e.g., spam class) into one big “class c” text. Then we use the frequency of the word (e.g., <em>urgent</em> below) in this concatenated document to give a (maximum likelihood) estimate of the probability:</p>
<div class="math notranslate nohighlight">
\[P(\text{urgent} \mid \text{spam}) = \frac{Count(\text{urgent}, \text{spam})}{\sum_{w \in vocabulary} Count(w, \text{spam})}\]</div>
<div class="math notranslate nohighlight">
\[P(\text{urgent} \mid \text{spam}) = \frac{\text{how often urgent occurs with spam}}{\text{total number of tokens (all occurrences of all words) in spam}}\]</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="naive-bayes-with-continuous-features">
<h2>Naive Bayes with continuous features<a class="headerlink" href="#naive-bayes-with-continuous-features" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>We can use Gaussian Naive Bayes if you have continuous features. (<a class="reference external" href="https://www.youtube.com/watch?v=H3EjCKtlVog">Watch this</a>).</p></li>
<li><p>Here is the general idea of Gaussian naive Bayes:</p>
<ul>
<li><p>Assume each feature is normally distributed.</p></li>
<li><p>Calculate the mean (<span class="math notranslate nohighlight">\(\mu_k\)</span>) and standard deviation (<span class="math notranslate nohighlight">\(\sigma_k\)</span>) for each feature for each class.</p></li>
<li><p>Use the following equation to calculate the likelihood of observing feature value <span class="math notranslate nohighlight">\(v\)</span> in class <span class="math notranslate nohighlight">\(C_k\)</span></p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../_images/gaus_nb.png" /></p>
<!-- <center>
<img src='./img/gaus_nb.png' width="400">
</center>
 -->
<p>Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Gaussian_naive_Bayes">Wikipedia</a></p>
<p>Let’s work through a toy example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Weight (in grams)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">162</span><span class="p">,</span> <span class="mi">163</span><span class="p">,</span> <span class="mi">164</span><span class="p">],</span>
    <span class="s1">&#39;Sugar Content (in %)&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">,</span> <span class="mf">10.2</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mf">19.5</span><span class="p">,</span> <span class="mf">18.8</span><span class="p">],</span>
    <span class="s1">&#39;Fruit&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Apple&#39;</span><span class="p">,</span> <span class="s1">&#39;Apple&#39;</span><span class="p">,</span> <span class="s1">&#39;Apple&#39;</span><span class="p">,</span> <span class="s1">&#39;Apple&#39;</span><span class="p">,</span> <span class="s1">&#39;Orange&#39;</span><span class="p">,</span> <span class="s1">&#39;Orange&#39;</span><span class="p">,</span> <span class="s1">&#39;Orange&#39;</span><span class="p">,</span> <span class="s1">&#39;Orange&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight (in grams)</th>
      <th>Sugar Content (in %)</th>
      <th>Fruit</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>100</td>
      <td>10.0</td>
      <td>Apple</td>
    </tr>
    <tr>
      <th>1</th>
      <td>105</td>
      <td>11.0</td>
      <td>Apple</td>
    </tr>
    <tr>
      <th>2</th>
      <td>103</td>
      <td>10.5</td>
      <td>Apple</td>
    </tr>
    <tr>
      <th>3</th>
      <td>101</td>
      <td>10.2</td>
      <td>Apple</td>
    </tr>
    <tr>
      <th>4</th>
      <td>160</td>
      <td>18.0</td>
      <td>Orange</td>
    </tr>
    <tr>
      <th>5</th>
      <td>162</td>
      <td>19.0</td>
      <td>Orange</td>
    </tr>
    <tr>
      <th>6</th>
      <td>163</td>
      <td>19.5</td>
      <td>Orange</td>
    </tr>
    <tr>
      <th>7</th>
      <td>164</td>
      <td>18.8</td>
      <td>Orange</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_gaussians</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="c1"># custom-defined function in code/plotting_functions.py</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/fe8c047d7bb8ac50cdd5e684a6f4afda7a8bca7fdaf805d28f1e8d38efd98994.png" src="../_images/fe8c047d7bb8ac50cdd5e684a6f4afda7a8bca7fdaf805d28f1e8d38efd98994.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Weight (in grams)</th>
      <th>Sugar Content (in %)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>8.000000</td>
      <td>8.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>132.250000</td>
      <td>14.625000</td>
    </tr>
    <tr>
      <th>std</th>
      <td>32.123645</td>
      <td>4.517506</td>
    </tr>
    <tr>
      <th>min</th>
      <td>100.000000</td>
      <td>10.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>102.500000</td>
      <td>10.425000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>132.500000</td>
      <td>14.500000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>162.250000</td>
      <td>18.850000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>164.000000</td>
      <td>19.500000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_toy</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fruit&#39;</span><span class="p">])</span>
<span class="n">y_toy</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Fruit&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s say we have a test fruit with a weight of 106 grams and a sugar content of 11% and we want to predict the class.
We’ll compute the likelihood of these values for both fruits using the Gaussian pdf:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gaussian_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variance</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">variance</span><span class="p">))</span>

<span class="c1"># Compute conditional probabilities for the given observation</span>
<span class="n">observed_weight</span> <span class="o">=</span> <span class="mi">106</span>
<span class="n">observed_sugar_content</span> <span class="o">=</span> <span class="mi">11</span>

<span class="n">likelihoods</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">fruit</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;Apple&#39;</span><span class="p">,</span> <span class="s1">&#39;Orange&#39;</span><span class="p">]:</span>
    <span class="n">likelihoods</span><span class="p">[</span><span class="n">fruit</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">observed_value</span> <span class="ow">in</span> <span class="p">[(</span><span class="s1">&#39;Weight (in grams)&#39;</span><span class="p">,</span> <span class="n">observed_weight</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;Sugar Content (in %)&#39;</span><span class="p">,</span> <span class="n">observed_sugar_content</span><span class="p">)]:</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Fruit&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fruit</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">variance</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Fruit&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">fruit</span><span class="p">][</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">var</span><span class="p">()</span>
        <span class="n">likelihoods</span><span class="p">[</span><span class="n">fruit</span><span class="p">][</span><span class="n">feature</span> <span class="o">+</span> <span class="s2">&quot;=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">observed_value</span><span class="p">)]</span> <span class="o">=</span> <span class="n">gaussian_pdf</span><span class="p">(</span><span class="n">observed_value</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
        
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">likelihoods</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Apple</th>
      <th>Orange</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Weight (in grams)=106</th>
      <td>0.043052</td>
      <td>6.345802e-237</td>
    </tr>
    <tr>
      <th>Sugar Content (in %)=11</th>
      <td>0.382788</td>
      <td>4.368922e-35</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><span class="math notranslate nohighlight">\(p(Apple \mid  \text{Weight (in grams) = 106, Sugar Content (in \%) = 11}) = \)</span>
<span class="math notranslate nohighlight">\( p(Apple) \times p(\text{Weight (in grams} = 106 \mid c = \text{Apple})  \times  p(\text{Sugar Content (in \%)} = 11 \mid c = \text{Apple})\)</span></p>
<p><span class="math notranslate nohighlight">\(p(Orange \mid  \text{Weight (in grams) = 106, Sugar Content (in \%) = 11}) = \)</span>
<span class="math notranslate nohighlight">\( p(Orange) \times p(\text{Weight (in grams} = 106 \mid c = \text{Orange})  \times  p(\text{Sugar Content (in \%)} = 11 \mid c = \text{Orange})\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_toy</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_toy</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">GaussianNB</label><div class="sk-toggleable__content"><pre>GaussianNB()</pre></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">var_</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">theta_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([[3.6875009, 0.1418759],
        [2.1875009, 0.2918759]]),
 array([[102.25 ,  10.425],
        [162.25 ,  18.825]]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">class_prior_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.5, 0.5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">106.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;Apple&#39;], dtype=&#39;&lt;U6&#39;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Gaussian naive Bayes assumes normality</p>
<ul>
<li><p>If the features are not normally distributed transform the data to make them more normal</p></li>
<li><p>Scikit-learn provides the <code class="docutils literal notranslate"><span class="pre">PowerTransformer()</span></code> for this process</p></li>
<li><p>From the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer">docs</a>: “<em>…Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like.</em>”</p></li>
</ul>
</li>
</ul>
<p><br><br><br><br></p>
<section id="optional-the-generative-model-behind-naive-bayes">
<h3>(Optional) The generative model behind naive Bayes<a class="headerlink" href="#optional-the-generative-model-behind-naive-bayes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Naive Bayes is a <strong>generative model</strong> because it’s modeling the joint distribution over the features <span class="math notranslate nohighlight">\(X\)</span> and labels <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>In the next lecture, we’ll look at a <strong>discriminative model</strong>, which models <span class="math notranslate nohighlight">\(P(y\mid X)\)</span> rather than <span class="math notranslate nohighlight">\(P(X,y)\)</span>.</p></li>
<li><p>Right now just get familiar with these terms. We’ll talk more about them later in the program.</p></li>
</ul>
<p>It’s weird to think of the features as random, but we’re saying they come from those Gaussians above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">class_prior_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.5, 0.5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">var_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[3.6875009, 0.1418759],
       [2.1875009, 0.2918759]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">theta_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[102.25 ,  10.425],
       [162.25 ,  18.825]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">gen_gaussian_nb</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">sigmas</span><span class="p">,</span> <span class="n">p_y</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
    <span class="c1"># means:  a 2D array of size num_classes by num features containing the means of the Gaussians</span>
    <span class="c1"># sigmas: a 2D array of size num_classes by num features containing the standard deviations of the Gaussians</span>
    <span class="c1"># p_y   : a 1D array of length num_classes containing the probability of each class</span>
    <span class="c1"># n     : an integer for the number of samples (&quot;size of sample&quot;) to generate</span>

    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of classes</span>
    <span class="n">d</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># number of features</span>
    <span class="n">X_generated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="n">y_generated</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="c1"># pick Canadian city/USA city</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p_y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
            <span class="n">X_generated</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> <span class="n">sigmas</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span>
        <span class="n">y_generated</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>    
        
    <span class="k">return</span> <span class="n">X_generated</span><span class="p">,</span> <span class="n">y_generated</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_generated</span><span class="p">,</span> <span class="n">y_generated</span> <span class="o">=</span> <span class="n">gen_gaussian_nb</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">theta_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">var_</span><span class="p">),</span> <span class="n">model</span><span class="o">.</span><span class="n">class_prior_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_generated</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_generated</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_generated</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Weight (in grams)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sugar Content (in %)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a416af3483290ed887dc9b61a7fdb00b40ba5f7594bb4ee2b00b3ed892bce9e4.png" src="../_images/a416af3483290ed887dc9b61a7fdb00b40ba5f7594bb4ee2b00b3ed892bce9e4.png" />
</div>
</div>
<p>We can experiment with different priors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_generated</span><span class="p">,</span> <span class="n">y_generated</span> <span class="o">=</span> <span class="n">gen_gaussian_nb</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">theta_</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">var_</span><span class="p">),</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.8</span><span class="p">])</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_generated</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_generated</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_generated</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Weight (in grams)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sugar Content (in %)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d5303f0d57cfa405fc126915223999a827332ac299c8906d595877b363be25f6.png" src="../_images/d5303f0d57cfa405fc126915223999a827332ac299c8906d595877b363be25f6.png" />
</div>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="optional-naive-bayes-on-multi-class-problems">
<h2>(Optional) Naive Bayes on multi-class problems<a class="headerlink" href="#optional-naive-bayes-on-multi-class-problems" title="Permalink to this heading">#</a></h2>
<p>Let’s use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s inbuilt 20 newsgroups dataset. It comprises around 18,000 newsgroups posts on 20 topics. So it’s a 20-class classification problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_20newsgroups</span>

<span class="n">newsgroups_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;headers&quot;</span><span class="p">,</span> <span class="s2">&quot;footers&quot;</span><span class="p">,</span> <span class="s2">&quot;quotes&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">newsgroups_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;headers&quot;</span><span class="p">,</span> <span class="s2">&quot;footers&quot;</span><span class="p">,</span> <span class="s2">&quot;quotes&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">X_news_train</span><span class="p">,</span> <span class="n">y_news_train</span> <span class="o">=</span> <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_news_test</span><span class="p">,</span> <span class="n">y_news_test</span> <span class="o">=</span> <span class="n">newsgroups_test</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">newsgroups_test</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the 20 topics (targets or classes) in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;alt.atheism&#39;,
 &#39;comp.graphics&#39;,
 &#39;comp.os.ms-windows.misc&#39;,
 &#39;comp.sys.ibm.pc.hardware&#39;,
 &#39;comp.sys.mac.hardware&#39;,
 &#39;comp.windows.x&#39;,
 &#39;misc.forsale&#39;,
 &#39;rec.autos&#39;,
 &#39;rec.motorcycles&#39;,
 &#39;rec.sport.baseball&#39;,
 &#39;rec.sport.hockey&#39;,
 &#39;sci.crypt&#39;,
 &#39;sci.electronics&#39;,
 &#39;sci.med&#39;,
 &#39;sci.space&#39;,
 &#39;soc.religion.christian&#39;,
 &#39;talk.politics.guns&#39;,
 &#39;talk.politics.mideast&#39;,
 &#39;talk.politics.misc&#39;,
 &#39;talk.religion.misc&#39;]
</pre></div>
</div>
</div>
</div>
<section id="let-s-try-dummyclassifier">
<h3>Let’s try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code><a class="headerlink" href="#let-s-try-dummyclassifier" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">X_news_train</span><span class="p">,</span> <span class="n">y_news_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.002476</td>
      <td>0.001425</td>
      <td>0.053027</td>
      <td>0.053033</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.001438</td>
      <td>0.000262</td>
      <td>0.053027</td>
      <td>0.053033</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.001139</td>
      <td>0.000248</td>
      <td>0.053027</td>
      <td>0.053033</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.001165</td>
      <td>0.000242</td>
      <td>0.053027</td>
      <td>0.053033</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.001144</td>
      <td>0.000234</td>
      <td>0.053050</td>
      <td>0.053027</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The scores are very low because we have 20 different classes!!</p>
</section>
<section id="let-s-try-multinomialnb">
<h3>Let’s try <code class="docutils literal notranslate"><span class="pre">MultinomialNB</span></code><a class="headerlink" href="#let-s-try-multinomialnb" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_multi</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">CountVectorizer</span><span class="p">(),</span> <span class="n">MultinomialNB</span><span class="p">())</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_multi</span><span class="p">,</span> <span class="n">X_news_train</span><span class="p">,</span> <span class="n">y_news_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.935332</td>
      <td>0.188610</td>
      <td>0.583738</td>
      <td>0.741465</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.809867</td>
      <td>0.199714</td>
      <td>0.570040</td>
      <td>0.739587</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.820616</td>
      <td>0.182462</td>
      <td>0.583297</td>
      <td>0.749309</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.845887</td>
      <td>0.173523</td>
      <td>0.571365</td>
      <td>0.731963</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.814050</td>
      <td>0.173680</td>
      <td>0.588859</td>
      <td>0.739063</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The validation scores are low but much better than the <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code>.</p>
<p>Let’s try to predict on an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_multi</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_news_train</span><span class="p">,</span> <span class="n">y_news_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">X_news_test</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">pipe_multi</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">X_news_test</span><span class="p">[</span><span class="mi">100</span><span class="p">]])[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="s2">&quot;Prediction index: </span><span class="si">%d</span><span class="s2"> and prediction class: </span><span class="si">%s</span><span class="s2">&quot;</span>
    <span class="o">%</span> <span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">prediction</span><span class="p">])</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Hello All!

    It is my understanding that all True-Type fonts in Windows are loaded in
prior to starting Windows - this makes getting into Windows quite slow if you
have hundreds of them as I do.  First off, am I correct in this thinking -
secondly, if that is the case - can you get Windows to ignore them on boot and
maybe make something like a PIF file to load them only when you enter the
applications that need fonts?  Any ideas?


Chris
Prediction index: 5 and prediction class: comp.windows.x
</pre></div>
</div>
</div>
</div>
<p>And here are prediction probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_multi</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="n">X_news_test</span><span class="p">[</span><span class="mi">100</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[7.26904475e-19, 1.48743104e-10, 4.16538604e-28, 2.96340768e-10,
        1.91391647e-14, 1.00000000e+00, 4.34194388e-28, 2.40916532e-18,
        9.74009600e-22, 1.72603033e-31, 5.61184330e-31, 8.76853929e-14,
        1.00705690e-16, 2.70331089e-13, 2.36762710e-21, 5.91222397e-16,
        3.61589511e-15, 1.49715839e-13, 2.92789387e-16, 1.20413840e-24]])
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="summary-and-final-remarks">
<h2>Summary and final remarks<a class="headerlink" href="#summary-and-final-remarks" title="Permalink to this heading">#</a></h2>
<section id="general-comments-on-naive-bayes">
<h3>General comments on naive Bayes<a class="headerlink" href="#general-comments-on-naive-bayes" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Family of probabilistic classifiers</p></li>
<li><p>GausianNB is used on very high dimensional numeric data</p></li>
<li><p>BernoulliNB and MultinomialNB are used for sparse data (e.g., text data).</p>
<ul>
<li><p>MultinomialNB usually performs better than BernoulliNB, especially when you are working with large documents.</p></li>
<li><p>Complexity hyperparameter: <code class="docutils literal notranslate"><span class="pre">alpha</span></code></p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>A fast and robust way to learn the corresponding parameters</p></li>
<li><p>Training procedure is easy to understand</p></li>
<li><p>Surprising accuracy considering its simplicity</p></li>
<li><p>Scales great; learning a naive Bayes classifier is just a matter of counting how many times each attribute co-occurs with each class</p></li>
<li><p>To avoid underflow, all the calculations are done with log likelihoods.</p></li>
<li><p>Can be easily used for multi-class classification</p></li>
</ul>
<ul class="simple">
<li><p>It’s closely related to linear classifiers, which we’ll see in the next lecture.</p>
<ul>
<li><p>When we take the logarithms, the products turn into summations.</p></li>
</ul>
</li>
<li><p>Can provides a informative set of features from which to predict the class (next class)</p></li>
</ul>
<ul class="simple">
<li><p>Often provide generalization performance that is slightly worse</p></li>
<li><p>Assumes that spammers generate e-mails by picking words at random. It means that sentences have no syntax and content. Is that a fair assumption?</p>
<ul>
<li><p>oversimplification</p></li>
<li><p>sometimes the best theories are the most oversimplified 🤷🏻‍♀️</p></li>
</ul>
</li>
<li><p>Although naive Bayes is known as a decent classifier, it is known to be a <strong>bad estimator</strong>, so the probability outputs from <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> are not to be taken too seriously.</p></li>
</ul>
<p><img alt="" src="../_images/eva-seeyou.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "571"
        },
        kernelOptions: {
            name: "571",
            path: "./lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = '571'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="06_hyperparameter-optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture 6: Hyperparameter Optimization and Optimization Bias</p>
      </div>
    </a>
    <a class="right-next"
       href="08_linear-models.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 8: Linear Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lecture-learning-objectives">Lecture learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivating-example">Motivating example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-spam-ham">Example: spam/ham</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#countvectorizer-to-get-bag-of-words-bow-representation"><code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> to get bag-of-words (BOW) representation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-1-decision-trees-quantifying-the-level-of-certainty-of-predictions">Exercise 7.1.1 Decision trees: Quantifying the level of certainty of predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-1-knns-quantifying-the-level-of-certainty-of-predictions">Exercise 7.1.1 KNNs: Quantifying the level of certainty of predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-7-1-3-calculating-p-spam-mid-message-and-p-ham-mid-message">Exercise 7.1.3 Calculating <span class="math notranslate nohighlight">\(P(spam \mid message)\)</span> and <span class="math notranslate nohighlight">\(P(ham \mid message)\)</span></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior-probabilities-p-text-spam-and-p-text-ham">The prior probabilities <span class="math notranslate nohighlight">\(P(\text{spam})\)</span> and <span class="math notranslate nohighlight">\(P(\text{ham})\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-text-message"><span class="math notranslate nohighlight">\(P(\text{message})\)</span></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#p-text-message-mid-text-spam"><span class="math notranslate nohighlight">\(P(\text{message} \mid \text{spam})\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes">Naive Bayes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#svc-classifier-for-spam-detection">SVC classifier for spam detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sklearn-naive-bayes-classifier"><code class="docutils literal notranslate"><span class="pre">sklearn</span></code> naive Bayes classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-predict">Naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-prior-probabilities">Computing prior probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-conditional-probabilities">Computing conditional probabilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-for-the-target-spam">Conditional probabilities for the target spam</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities-for-the-target-ham">Conditional probabilities for the target ham</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-naive-bayes-predict">Summary of naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-naive-bayes-predict">Summary naive Bayes <code class="docutils literal notranslate"><span class="pre">predict</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">❓❓ Questions for you</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-7-2">(iClicker) Exercise 7.2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-classifier-fit">Naive Bayes classifier <code class="docutils literal notranslate"><span class="pre">fit</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-probabilities">Prior probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probabilities">Conditional probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#laplace-smoothing">Laplace smoothing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simplest-solution-laplace-smoothing">A simplest solution: Laplace smoothing</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alpha-hyperparameter-and-the-fundamental-tradeoff"><code class="docutils literal notranslate"><span class="pre">alpha</span></code> hyperparameter and the fundamental tradeoff</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-details-on-the-formulas">(Optional) Details on the formulas</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#naive-bayes-with-continuous-features">Naive Bayes with continuous features</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-the-generative-model-behind-naive-bayes">(Optional) The generative model behind naive Bayes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-naive-bayes-on-multi-class-problems">(Optional) Naive Bayes on multi-class problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-try-dummyclassifier">Let’s try <code class="docutils literal notranslate"><span class="pre">DummyClassifier</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#let-s-try-multinomialnb">Let’s try <code class="docutils literal notranslate"><span class="pre">MultinomialNB</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-final-remarks">Summary and final remarks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-comments-on-naive-bayes">General comments on naive Bayes</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>